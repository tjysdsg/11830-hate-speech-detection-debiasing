{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "J0Ed0oVVhdqF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  with open(filename, 'r') as json_file:\n",
        "      json_list = list(json_file)\n",
        "\n",
        "  dic = []\n",
        "  for line in json_list:\n",
        "    dic.append(json.loads(line))\n",
        "\n",
        "  return dic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw = read_data('train.jsonl')\n",
        "dev_raw = read_data('dev.jsonl')\n",
        "test_raw = read_data('test.jsonl')"
      ],
      "metadata": {
        "id": "Vg4Wz2kGkz9-"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys-2i7RFk2R_",
        "outputId": "50ef5114-2301-4a76-a13e-4d6630de4a39"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text_id': 29620252,\n",
              " 'Text': 'test fuck ejaculate mother cheez whiz ',\n",
              " 'purity': 0,\n",
              " 'harm': 0,\n",
              " 'im': 1,\n",
              " 'cv': 0,\n",
              " 'ex': 0,\n",
              " 'degradation': 0,\n",
              " 'fairness': 0,\n",
              " 'hd': 0,\n",
              " 'mph': 0,\n",
              " 'loyalty': 0,\n",
              " 'care': 0,\n",
              " 'betrayal': 0,\n",
              " 'gen': 0,\n",
              " 'cheating': 0,\n",
              " 'subversion': 0,\n",
              " 'rel': 0,\n",
              " 'sxo': 0,\n",
              " 'rae': 0,\n",
              " 'nat': 0,\n",
              " 'pol': 0,\n",
              " 'authority': 0,\n",
              " 'vo': 1,\n",
              " 'idl': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://strubell.github.io/teaching/11-830/assignments/files/identity_labels.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pphmX6RCpR0B",
        "outputId": "0e21867e-93e2-426f-9f8a-22194380453a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 00:50:16--  https://strubell.github.io/teaching/11-830/assignments/files/identity_labels.txt\n",
            "Resolving strubell.github.io (strubell.github.io)... 185.199.110.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to strubell.github.io (strubell.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1047 (1.0K) [text/plain]\n",
            "Saving to: ‘identity_labels.txt’\n",
            "\n",
            "\ridentity_labels.txt   0%[                    ]       0  --.-KB/s               \ridentity_labels.txt 100%[===================>]   1.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-21 00:50:16 (24.9 MB/s) - ‘identity_labels.txt’ saved [1047/1047]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_BSW(filenames):\n",
        "\n",
        "  identity_list = []\n",
        "  with open(filenames[0], 'r') as f:\n",
        "    lines= f.readlines()\n",
        "    for line in lines:\n",
        "      identity_list.append(line.rstrip())\n",
        "\n",
        "  identity_list_2 = list(pd.read_csv(filenames[1], sep='\\t')['muslim'])\n",
        "  identity_list_2.append('muslim')\n",
        "\n",
        "  identity_list_3 = list(pd.read_csv(filenames[2], sep='\\t')['jew'])\n",
        "  identity_list_2.append('jew')\n",
        "  \n",
        "  result = list(set(identity_list + identity_list_2 + identity_list_3))\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "BCRnxiBKk236"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identities = read_BSW(['identity_labels.txt', 'identity.csv', 'identity_ws_new.csv'])\n",
        "len(identities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Az-0PhYnXnq",
        "outputId": "207836a7-1e93-4653-c305-14ee9e67a8d9"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "VPIqvq-ZqRhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "detokenizer = TreebankWordDetokenizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_09sASaqS0N",
        "outputId": "0b0c444f-6e12-4c5a-839b-1be4120aaeda"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def to_pos(data):\n",
        "  for k in tqdm(range(len(data))):\n",
        "    sent = data[k][\"Text\"]\n",
        "    text = word_tokenize(sent.lower())\n",
        "\n",
        "    for i in range(len(text)):\n",
        "      if text[i] in identities:\n",
        "        text[i] = nlp(text[i])[0].pos_\n",
        "        \n",
        "    sent = detokenizer.detokenize(text)\n",
        "    data[k]['Text'] = sent\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "A6riQ7R6sey-"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ner(data):\n",
        "  for k in tqdm(range(len(data))):\n",
        "    sent = data[k][\"Text\"]\n",
        "    entities = nlp(sent.lower())\n",
        "    text = [t.text if not t.ent_type_ else t.ent_type_ for t in entities]\n",
        "    sent = detokenizer.detokenize(text)\n",
        "    data[k]['Text'] = sent\n",
        "  return data"
      ],
      "metadata": {
        "id": "XEj47F7QyE2t"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_raw.copy()\n",
        "dev_data = dev_raw.copy()\n",
        "\n",
        "train_data_pos = to_pos(train_data) # 963 969 971 973 980 986 997 998 1000 1006\n",
        "dev_data_pos = to_pos(dev_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stMlaVLSuD-u",
        "outputId": "7fb4d11c-862d-49be-aa32-41ea1a33ae78"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20651/20651 [01:42<00:00, 201.27it/s]\n",
            "100%|██████████| 2581/2581 [00:01<00:00, 1906.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_raw.copy()\n",
        "dev_data = dev_raw.copy()\n",
        "\n",
        "train_data_ner = to_ner(train_data) # 5 6 7 8 9 11 12 23 24 25\n",
        "dev_data_ner = to_ner(dev_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5va0wzU4Y41",
        "outputId": "9da0f2f2-c9cb-43a5-a400-f2ac77a942d2"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20651/20651 [04:46<00:00, 71.96it/s]\n",
            "100%|██████████| 2581/2581 [00:35<00:00, 73.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_pos[963]['Text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Uod2hkI0uziV",
        "outputId": "8baea4b9-43bc-40bf-d634-cc03ba585e0b"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"a handful of intelligent & dedicated noun & noun can - & will - change the course of history . nineteen years ago, a few laid the foundation for what is now called 'populist nationalism .' others among us began to expose large scale corruption in government 100 such will rock the world to its core\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_ner[6]['Text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lu1OR-KA_sog",
        "outputId": "bb2d4f5d-0ef1-421a-c3fd-d33d9a23dfa1"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"didn't want the non - existent propn dirt .... just wanted to pay to find out how bad the propn screwed up .... that's all .... really...no really .... propn: punct\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sent = \"How can these Liberal banks be allowed to dictate what LEGAL items consumers are allowed to purchase? What if the Liberals decide they don't like white shirts? And what about the debit card accounts that are prepaid with your own money? These companies should be REQUIRED BY LAW to complete ANY LEGAL transaction if they want a license to serve the public.\"\n",
        "sent = 'She is a black woman'\n",
        "sent = 'Mohan is a rock star of Hollywood'\n",
        "\n",
        "# text = word_tokenize(sent.lower())\n",
        "entities = nlp(sent)\n",
        "text = [t.text if not t.ent_type_ else t.ent_type_ for t in entities]\n",
        "sent = detokenizer.detokenize(text)\n",
        "sent"
      ],
      "metadata": {
        "id": "x8MthLTNtrNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_file(filename, data):\n",
        "  with open(filename, 'w') as f:\n",
        "    for i in range(len(data)):\n",
        "      json.dump(data[i], f)\n",
        "      f.write('\\n')"
      ],
      "metadata": {
        "id": "46YjoOkz3gw-"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_file('train_pos.jsonl', train_data_pos)\n",
        "dump_file('dev_pos.jsonl', dev_data_pos)"
      ],
      "metadata": {
        "id": "LgH2bBXz6cSh"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_file('train_ner.jsonl', train_data_ner)\n",
        "dump_file('dev_ner.jsonl', dev_data_ner)"
      ],
      "metadata": {
        "id": "o5DP00Rg8OF5"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NEbVOb2_3xe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}